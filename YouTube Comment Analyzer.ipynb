{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOK8wg2QwtJfbbaXFc9cJer",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HaddyDev/MY-AI-projects-/blob/main/YouTube%20Comment%20Analyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install --upgrade google-api-python-client google-auth google-auth-oauthlib google-auth-httplib2\n",
        "!pip install gradio nltk textblob transformers\n",
        "import gradio as gr\n",
        "import re\n",
        "import nltk\n",
        "import google.auth\n",
        "import matplotlib.pyplot as plt\n",
        "from googleapiclient.discovery import build\n",
        "from textblob import TextBlob\n",
        "from transformers import pipeline\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('stopwords')\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Initialize Sentiment and Emotion Models\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "emotion_model = pipeline(\"text-classification\", model=\"bhadresh-savani/distilbert-base-uncased-emotion\", return_all_scores=True)\n",
        "\n",
        "# YouTube API Key (Replace with your own)\n",
        "YOUTUBE_API_KEY = \"AIzaSyBh2zlKSYCT09GsYe1EUtftz255kW2SZnk\"\n",
        "\n",
        "# Function to extract Video ID from URL\n",
        "def extract_video_id(video_url):\n",
        "    patterns = [\n",
        "        r\"(?:https?:\\/\\/)?(?:www\\.)?youtu\\.be\\/([a-zA-Z0-9_-]+)\",\n",
        "        r\"(?:https?:\\/\\/)?(?:www\\.)?youtube\\.com\\/watch\\?v=([a-zA-Z0-9_-]+)\",\n",
        "    ]\n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, video_url)\n",
        "        if match:\n",
        "            return match.group(1)\n",
        "    return None  # Invalid URL\n",
        "\n",
        "# Function to fetch YouTube comments\n",
        "def fetch_youtube_comments(video_url):\n",
        "    video_id = extract_video_id(video_url)\n",
        "    if not video_id:\n",
        "        return \"âŒ Invalid YouTube URL.\", None\n",
        "    try:\n",
        "        youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)\n",
        "        response = youtube.commentThreads().list(\n",
        "            part=\"snippet\",\n",
        "            videoId=video_id,\n",
        "            textFormat=\"plainText\",\n",
        "            maxResults=50\n",
        "        ).execute()\n",
        "        comments = [(item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"],\n",
        "                     item[\"snippet\"][\"topLevelComment\"][\"snippet\"].get(\"likeCount\", 0),\n",
        "                     item.get(\"snippet\", {}).get(\"totalReplyCount\", 0))\n",
        "                    for item in response[\"items\"]]\n",
        "        return comments, video_id\n",
        "    except Exception as e:\n",
        "        return f\"âŒ Error: {str(e)}\", None\n",
        "\n",
        "# Function to analyze sentiment\n",
        "def analyze_sentiment(text):\n",
        "    sentiment_score = sia.polarity_scores(text)['compound']\n",
        "    if sentiment_score > 0.05:\n",
        "        return \"Positive\"\n",
        "    elif sentiment_score < -0.05:\n",
        "        return \"Negative\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "# Function to analyze emotion\n",
        "def analyze_emotion(text):\n",
        "    emotions = emotion_model(text)\n",
        "    highest_emotion = max(emotions[0], key=lambda x: x[\"score\"])\n",
        "    return highest_emotion[\"label\"].capitalize()\n",
        "\n",
        "# Function to generate Pie Charts\n",
        "def generate_pie_chart(data, title):\n",
        "    labels, values = zip(*data.items())\n",
        "    fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    ax.pie(values, labels=labels, autopct='%1.1f%%')\n",
        "    ax.set_title(title)\n",
        "    return fig\n",
        "\n",
        "# Function to generate Word Cloud\n",
        "def generate_wordcloud(text_data):\n",
        "    words = ' '.join(text_data)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    wordcloud = WordCloud(stopwords=stop_words, background_color='white').generate(words)\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.imshow(wordcloud, interpolation='bilinear')\n",
        "    ax.axis(\"off\")\n",
        "    return fig\n",
        "\n",
        "# Main Analysis Function\n",
        "def youtube_sentiment_analysis(video_url):\n",
        "    comments_data, video_id = fetch_youtube_comments(video_url)\n",
        "    if isinstance(comments_data, str):\n",
        "        return comments_data, None, None, None, None, None, None\n",
        "\n",
        "    comments, likes, replies = zip(*comments_data)\n",
        "    sentiments = [analyze_sentiment(comment) for comment in comments]\n",
        "    emotions = [analyze_emotion(comment) for comment in comments]\n",
        "    sentiment_counts = Counter(sentiments)\n",
        "    emotion_counts = Counter(emotions)\n",
        "\n",
        "    # Identify Most Liked & Most Replied Comments\n",
        "    most_liked = sorted(comments_data, key=lambda x: x[1], reverse=True)[:3]\n",
        "    most_replied = sorted(comments_data, key=lambda x: x[2], reverse=True)[:3]\n",
        "\n",
        "    # Generate Charts\n",
        "    sentiment_pie = generate_pie_chart(sentiment_counts, \"Sentiment Analysis\")\n",
        "    emotion_pie = generate_pie_chart(emotion_counts, \"Emotion Analysis\")\n",
        "    wordcloud_img = generate_wordcloud(comments)\n",
        "\n",
        "    return (\n",
        "\n",
        "        sentiment_pie,\n",
        "        emotion_pie,\n",
        "        wordcloud_img,\n",
        "        \"\\n\".join([f'ðŸ‘ {c[1]} - \"{c[0]}\"' for c in most_liked]),\n",
        "        \"\\n\".join([f'ðŸ’¬ {c[2]} - \"{c[0]}\"' for c in most_replied])\n",
        "    )\n",
        "\n",
        "# Create Gradio UI\n",
        "iface = gr.Interface(\n",
        "    fn=youtube_sentiment_analysis,\n",
        "    inputs=gr.Textbox(label=\"Enter YouTube Video URL\"),\n",
        "    outputs=[\n",
        "\n",
        "        gr.Plot(label=\"Overalls Sentiment\"),\n",
        "        gr.Plot(label=\"Overalls Emotion \"),\n",
        "        gr.Plot(label=\"Frequent Keywords (Word Cloud)\"),\n",
        "        gr.Textbox(label=\"Top Most Liked Comments\"),\n",
        "        gr.Textbox(label=\"Top Most Replied Comments\")\n",
        "    ],\n",
        "    title=\"YouTube Comment Analyzer\",\n",
        "    description=\"Analyze comments from YouTube videos. Includes sentiment, emotions, keyword analysis, and engagement insights.\",\n",
        ")\n",
        "\n",
        "# Launch the App\n",
        "iface.launch(share=True)\n"
      ],
      "metadata": {
        "id": "EZqWFfIx_fEU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}